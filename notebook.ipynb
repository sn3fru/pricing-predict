{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neste Trabalho vamos criar um preditor de preços com regressões lineares.\n",
    "\n",
    "Ele não tem a intenção de ser um trabalho completo ou um estudo sobre a elasticide-preço, apenas uma introdução que passa por diversos problemas comuns nesse tipo de estudo e algumas possíveis soluções a esses problemas.\n",
    "\n",
    "O projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abc\\anaconda\\lib\\site-packages (from python-dateutil>=2->pandas)\n",
      "Requirement already satisfied: numpy in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abc\\anaconda\\lib\\site-packages (from sklearn)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn: started\n",
      "  Running setup.py bdist_wheel for sklearn: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ABC\\AppData\\Local\\pip\\Cache\\wheels\\d7\\db\\a3\\1b8041ab0be63b5c96c503df8e757cf205c2848cf9ef55f85e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.7.1 in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in c:\\users\\abc\\anaconda\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: scipy in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: statsmodels in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: pandas in c:\\users\\abc\\anaconda\\lib\\site-packages (from statsmodels)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas->statsmodels)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas->statsmodels)\n",
      "Requirement already satisfied: numpy>=1.7.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pandas->statsmodels)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abc\\anaconda\\lib\\site-packages (from python-dateutil>=2->pandas->statsmodels)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: protobuf>=3.2.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: backports.weakref==1.0rc1 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abc\\anaconda\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abc\\anaconda\\lib\\site-packages (from protobuf>=3.2.0->tensorflow)\n",
      "Requirement already satisfied: keras in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: theano in c:\\users\\abc\\anaconda\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\abc\\anaconda\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six in c:\\users\\abc\\anaconda\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\abc\\anaconda\\lib\\site-packages (from theano->keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\abc\\anaconda\\lib\\site-packages (from theano->keras)\n",
      "Requirement already satisfied: pysal in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.3 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pysal)\n",
      "Requirement already satisfied: scipy>=0.11 in c:\\users\\abc\\anaconda\\lib\\site-packages (from pysal)\n",
      "Collecting geopandas\n",
      "  Using cached geopandas-0.2.1-py2.py3-none-any.whl\n",
      "Collecting pyproj (from geopandas)\n",
      "  Using cached pyproj-1.9.5.1.tar.gz\n",
      "Collecting fiona (from geopandas)\n",
      "  Using cached Fiona-1.7.9.post1.tar.gz\n",
      "Collecting descartes (from geopandas)\n",
      "  Using cached descartes-1.1.0-py3-none-any.whl\n",
      "Collecting shapely (from geopandas)\n",
      "  Using cached Shapely-1.6.0.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\ABC\\AppData\\Local\\Temp\\pip-build-qmu2l_9k\\shapely\\setup.py\", line 80, in <module>\n",
      "        from shapely._buildcfg import geos_version_string, geos_version, \\\n",
      "      File \"C:\\Users\\ABC\\AppData\\Local\\Temp\\pip-build-qmu2l_9k\\shapely\\shapely\\_buildcfg.py\", line 200, in <module>\n",
      "        lgeos = CDLL(\"geos_c.dll\")\n",
      "      File \"C:\\Users\\ABC\\Anaconda\\lib\\ctypes\\__init__.py\", line 344, in __init__\n",
      "        self._handle = _dlopen(self._name, mode)\n",
      "    OSError: [WinError 126] Não foi possível encontrar o módulo especificado\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\ABC\\AppData\\Local\\Temp\\pip-build-qmu2l_9k\\shapely\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mplleaflet in c:\\users\\abc\\anaconda\\lib\\site-packages\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abc\\anaconda\\lib\\site-packages (from mplleaflet)\n",
      "Requirement already satisfied: six in c:\\users\\abc\\anaconda\\lib\\site-packages (from mplleaflet)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abc\\anaconda\\lib\\site-packages (from jinja2->mplleaflet)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install statsmodels\n",
    "!pip install keras\n",
    "!pip install pysal\n",
    "!pip install geopandas\n",
    "!pip install mplleaflet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Math, stat and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# sklearn for regressions\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# keras for deep learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# packages for geopatial regressions\n",
    "import pysal as ps\n",
    "import geopandas as gpd\n",
    "import mplleaflet as mpll\n",
    "\n",
    "# packages for viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\ABC\\\\OneDrive\\\\data\\\\pricing_predict\\\\data\\\\house_sales.csv'\n",
    "# path = '/Users/sn3fru/OneDrive/data/pricing_predict/data/house_sales.csv'\n",
    "# path = 'house_sales.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Analise exploratória\n",
    "\n",
    "#### Antes de mais nada vamos dar uma boa olhada no comportamento das distribuições cruzadas com a função pairplot do seaborn, com isso consigo ter uma boa ideia de quais variaveis são fortes candidatos a serem tratadas, quais tem alta correlação e quais são boas candidatas para dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue=\"condition\")\n",
    "# g.savefig('first-pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temos mais variaveis do que eu imaginava para fazer isso no olho, vamos ver como é a correlação geral dessas variaveis ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parece que muitas variaveis tem baixa correlação com as demais (pelo menos sem fazer algum tratamento) com isso vamos plotar apenas as 10 variaveis com maiores correlações com o preço.\n",
    "\n",
    "Estranho notar a latitude e longitude sendo sendo bastante correlacionado, isso só faria sentido se a amostra for toda de uma pequena região onde haveria um gradiente de preços. Se Fosse todo os EUA, esse feito linear se anularia.\n",
    "\n",
    "Vamos ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "cols = corrmat.nlargest(k, 'price')['price'].index\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale=1.15)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checamos que a base não tem valores nulos: (mas tem muitos valores zero onde deveria ser nulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### checamos quais as variaveis são muito assimétricas:\n",
    "numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uma breve olhada nos dados geograficos fornecidos já nos dizem muito sobre as possibilidades dessa base.\n",
    "\n",
    "Se tivessemos uma amostra de todo os EUA, não faria muito sentido usar autocorrelação espacial já que os efeitos são dissipados rápidamente, mas como estamos falando de uma area razoavelmente pequena, um único distrito, King County (em Washington), podemos não só usar as autocorrelações espaciais, como podemos usar os dados públicos como o censu americano ( https://www.census.gov ) e até dados de GIS (https://gis-kingcounty.opendata.arcgis.com/) para enriquecer nosso preditor de preços de venda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.arange(df.shape[0])\n",
    "np.random.shuffle(rids)\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.iloc[points[:500], :].plot(kind='scatter', x='longitude',\n",
    "                             y='latitude',s=30, linewidth=0, ax=ax);\n",
    "mpll.display(fig=f,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### mapa plotado no Tableau Public, os números nas regiões representam a quantidade unidades vendidas e a intensidade das cores é o preço médio das unidades conforme escala abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='C:\\\\Users\\\\ABC\\\\Desktop\\\\tableau-map.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por último vamos rodar uma simples regressão nos dados sem tratamento para ver como as variaveis se comportam ceteris paribus, ou seja, o valor dos betas, seus sinais e suas significancias estatisticas.\n",
    "\n",
    "Tem muita coisa para se analisar na regressão abaixo, mas vamos falar só do genérico e deixar para as próximas com variaveis tratadas.\n",
    "\n",
    "Temos o um R2 de 0.577 com muita assimetria e curtose elevada indicando residuos mal distribuidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function1 = '''\n",
    "price ~ num_bed\n",
    " + num_bath\n",
    " + size_house\n",
    " + size_lot\n",
    " + num_floors\n",
    " + is_waterfront\n",
    " + condition\n",
    " + size_basement\n",
    " + renovation_date\n",
    " + avg_size_neighbor_houses\n",
    " + avg_size_neighbor_lot\n",
    " + zip\n",
    "'''\n",
    "\n",
    "model1 = smf.ols(function1, df).fit()\n",
    "print(model1.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Engenharia de Variaveis\n",
    "\n",
    "A maior, mais díficil e mais importante etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embora os dados exibam um comportamento linear, a amostra é bastante heterocedastica.\n",
    "\n",
    "Analisando o primeiro pairplor, notamos séries 'explosivas', para torna-la homocedastica podemos aplicar diversas técnicas, usaremos o mais simples que é a aplicação de log nas duas variaveis pois além deconseguimos a variância constante, temos uma interpretação de elasticidade (taxa de variação) para os parâmetros.\n",
    "\n",
    "\n",
    "Abaixo a relação entre Preços e o tamanho da casa que imaginamos que tenha uma forte e positiva correlação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df['size_house'], y = df['price'])\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('Size', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descreverei como log-linearizaremos' as principais variaveis com mais detalhes e as demais será feito o procedimento (se necessário) sem explicações\n",
    "\n",
    "Não necessáriamente precisamos ter distribuições normais para nossas amostras, mas ela ter essa caracteristica permite que façamos analises não só mais eficientes mas principalmente mais robustas já que a maioria dos algoritmos de regressões que usaremos trazem betas significativos para qualquer distribuição apenas com o primeiro momento, mas não somos capazes de fazer testes de hipótese sem o segundo momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['price'] , fit=stats.norm);\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(df['price'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['price'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E após a aplicação de logaritmo natural na variavel ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = np.log(df['size_house']), y = np.log(df['price']))\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('Size', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.distplot(np.log(df['price']) , fit=stats.norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(np.log(df['price']))\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(np.log(df['price']), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A segunda variavel com maior correlação é a média de tamanho das casas da vizinhança.\n",
    "Uma forma alternatica de transformação essa informação em uma variavel explicativa interessante\n",
    "é considerar não exatamente o tamanho média das casas do bairro, mas o quanto estamos proximos ou longe\n",
    "da média das casas do bairro. Pois se a média é grande ou pequena mas nossa casa pode ser ainda maior ou ainda menor não fazendo a comparação relativa. Portanto vamos analisar a variavel, log-lineariza-la e criar uma nova que será uma proporção entre a venda e a sua média. \n",
    "\n",
    "Obs: Não faria sentido criar novas variaveis que fossem combinações lineares entre as variaveis do modelo, muitos softwares simplesmente não rodam por não conseguirem inveter a matriz de parametros (X) acusando multicolinearidade perfeita (ou em termos mais economicos, a variavel criada como combinação linear de outras não acrescenta nenhum novo poder explicativo ao modelo), neste casso criaremos um indice percentual para testarmos sua correlação com o preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df['avg_size_neighbor_houses'], y = df['price'])\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "res1 = stats.probplot(df['avg_size_neighbor_houses'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = np.log(df['avg_size_neighbor_houses']), y = np.log(df['price']))\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "res2 = stats.probplot(np.log(df['avg_size_neighbor_houses']), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embora a aplicação do log resolva facilmente alguns possíveis preblemas de distribuições 'mal comportadas' ele não resolve bem problemas de indices, pois a ideia é de que uma regressão log-log tenha variações constantes, dando a interpretação de 'ao variar nossa endógena em x% a exógena variará em \\beta %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df.copy(deep=True)\n",
    "df2['sizediff_relative_of_neighbor'] = (df2['avg_size_neighbor_houses']-df2['size_house'])/df2['size_house']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df2['avg_size_neighbor_houses'], y = df2['size_house'])\n",
    "plt.ylabel('size', fontsize=13)\n",
    "plt.xlabel('avg neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = np.log(df2['avg_size_neighbor_houses']), y = np.log(df2['size_house']))\n",
    "plt.ylabel('log size', fontsize=13)\n",
    "plt.xlabel('log avg neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "res0 = stats.probplot(df2['avg_size_neighbor_houses'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relação entre distancia percentual para a média e o Preço de venda\n",
    "obs: essa variavel não poderá ser usada nas regressões pois um dos inputs é a variavel exógena, usamos para entender a relação da variavel principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(df2['sizediff_relative_of_neighbor'], np.log(df2['price']), kind=\"kde\")\n",
    "plt.ylabel('log Price', fontsize=13)\n",
    "plt.xlabel('diff neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "res1 = stats.probplot(df2['sizediff_relative_of_neighbor'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['log_price'] = np.log(df2['price'])\n",
    "df2['log_size_house'] = np.log(df2['size_house'])\n",
    "df2['size_lot']\n",
    "df2['log_size_lot'] = np.log(df2['size_lot'])\n",
    "df2['log_neighbor_house'] = np.log(df2['avg_size_neighbor_houses'])\n",
    "df2['log_neighbor_lot'] = np.log(df2['avg_size_neighbor_lot'])\n",
    "df2['log_size_basement'] = np.log(df2['size_basement'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask1 = (df2['year_built'] != 0)\n",
    "z_valid = df2[mask1]\n",
    "\n",
    "df2['time_since_construction'] = 50\n",
    "df2.loc[mask1, 'time_since_construction'] = np.log(2016 - z_valid['year_built'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask2 = (df2['renovation_date'] != 0)\n",
    "z_valid = df2[mask2]\n",
    "\n",
    "df2['time_since_renovation'] = 50\n",
    "df2.loc[mask2, 'time_since_renovation'] = np.log(2016 - z_valid['renovation_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis candidatas a Categóricas (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"num_bath\", data=df2, kind=\"count\",\n",
    "                   palette=\"BuPu\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "df2['c_num_bath'] = df2.num_bath.replace({0:0, 0.50:0, 0.75:0,\n",
    "                                          1:1, 1.25:1, 1.50:1, 1.75:1,\n",
    "                                          2:2, 2.25:2, 2.50:2, 2.75:2,\n",
    "                                          3:3, 3.25:3, 3.50:3, 3.75:3,\n",
    "                                          4:4, 4.25:4, 4.50:4, 4.75:4,\n",
    "                                          5:5, 5.25:5, 5.50:5, 5.75:5,\n",
    "                                          6:5, 6.25:5, 6.50:5, 6.75:5,\n",
    "                                          7:5, 7.25:5, 7.50:5, 7.75:5,\n",
    "                                          8:5})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_bath\", data=df2, kind=\"count\",\n",
    "                   palette=\"BuPu\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "df2['c_num_partial_bath'] = df2.num_bath.replace({0:0, 0.25:1, 0.50:2, 0.75:3,\n",
    "                                                  1:0, 1.25:1, 1.50:2, 1.75:3,\n",
    "                                                  2:0, 2.25:1, 2.50:2, 2.75:3,\n",
    "                                                  3:0, 3.25:1, 3.50:2, 3.75:3,\n",
    "                                                  4:0, 4.25:1, 4.50:2, 4.75:3,\n",
    "                                                  5:0, 5.25:1, 5.50:2, 5.75:3,\n",
    "                                                  6:0, 6.25:1, 6.50:2, 6.75:3,\n",
    "                                                  7:0, 7.25:1, 7.50:2, 7.75:3,\n",
    "                                                  8:0})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_partial_bath\", data=df2, kind=\"count\",\n",
    "                   palette=\"BuPu\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"num_bed\", data=df2, kind=\"count\",\n",
    "                   palette=\"BuPu\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "df2['c_num_bed'] = df2.num_bed.replace({ 0:1, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:6, 8:6, 9:6, 10:6, 33:6})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_bed\", data=df2, kind=\"count\",\n",
    "                   palette=\"BuPu\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['c_num_floors'] = df2.num_floors.replace({ 1:1, 1.5:1, 2:2, 2.5:2, 3:3, 3.5:3})\n",
    "df2['c_num_partial_floors'] = df2.num_floors.replace({ 1:0, 1.5:1, 2:0, 2.5:1, 3:0, 3.5:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para evitarmos de criar dummies com o zip code que embora aumente o poder explicativo, deixamos os residuos novamente com o terceiro e quarto momentos ainda mais distantes da gaussiana, vamos criar um segundo dataframe com os valores médios agrupados por zip e usar como variaveis endógenas (aceitando a multicolineariedade ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spatial\n",
    "avg_zip = df2.groupby(df2['zip'])[['price', 'size_house', 'size_lot','latitude','longitude','year_built']].median()\n",
    "avg_zip.rename(columns={'price': 'zip_price',\n",
    "                        'size_house': 'zip_size_house',\n",
    "                        'size_lot':'zip_size_lot',\n",
    "                        'latitude': 'zip_latitude',\n",
    "                        'longitude': 'zip_longitude',\n",
    "                        'year_built': 'zip_year_built'}, inplace=True)\n",
    "avg_zip = avg_zip.reset_index()\n",
    "df2 = df2.merge(avg_zip, left_on='zip', right_on='zip', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['zip_log_price'] = np.log(df2['zip_price'])\n",
    "df2['zip_log_size_house'] = np.log(df2['zip_size_house'])\n",
    "df2['zip_log_size_lot'] = np.log(df2['zip_size_lot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos rodar novamente uma OLS simples para ver como as variaveis passaram a se comportar, seus testes conjuntos, assimetria, curtose e R2.\n",
    "\n",
    "Como evidenciado abaixo, conseguimos tanto aumentar o poder explicativo do modelo sem causar overfiting, além disso diminuimos muito a assimetria dos residuos e embora a curtose não seja baixa ela é 1/6 da inicial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function2 = '''\n",
    "log_price ~ sizediff_relative_of_neighbor\n",
    " + C(c_num_bed)\n",
    " + C(c_num_bath)\n",
    " + C(c_num_partial_bath)\n",
    " + log_size_house\n",
    " + log_size_lot\n",
    " + C(c_num_floors)\n",
    " + C(is_waterfront)\n",
    " + C(condition)\n",
    " + log_size_basement\n",
    " + time_since_renovation\n",
    " + time_since_construction\n",
    " + log_neighbor_house\n",
    " + log_neighbor_lot\n",
    " + np.log(zip_size_house)\n",
    " + np.log(zip_price)\n",
    " + np.log(zip_size_lot)\n",
    "'''\n",
    "\n",
    "model2 = smf.ols(function2, df2).fit()\n",
    "print(model2.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'condition': 'c_condition', 'is_waterfront':'c_is_waterfront'}, inplace=True)\n",
    "\n",
    "df3 = df2[['log_price', 'zip', 'latitude', 'longitude', 'sizediff_relative_of_neighbor',\n",
    "           'log_size_house', 'log_size_lot', 'log_neighbor_house', 'log_neighbor_lot',\n",
    "           'log_size_basement', 'time_since_construction', 'time_since_renovation',\n",
    "           'zip_log_size_house', 'zip_log_size_lot', 'zip_log_price','c_condition','c_num_floors',\n",
    "           'c_num_partial_floors', 'c_num_bath','c_num_partial_bath','c_num_bed', 'c_is_waterfront']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As variaveis com maiores e menores correlaçõoe com Preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = df3.corr()\n",
    "corr.sort_values([\"log_price\"], ascending = False, inplace = True)\n",
    "print(corr.log_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesse método vamos transformar nossas variaveis categóricas em dummies para permitir que a variação da variavel tenha efeitos diferentes sobre o preço, além de abrir a possibilidade de iterarmos as dummies com as variaveis continuar para flexibilizar o valor dos parametros, ou seja, torna-los condicionais a ter ou não a dummy diferente da regressão comum que considera o efeito ceteris paribus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoricals = ['c_condition','c_num_floors','c_num_partial_floors',\n",
    "                'c_num_bath','c_num_partial_bath','c_num_bed', 'c_is_waterfront', 'zip']\n",
    "\n",
    "def one_hot(df, cols):\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "df3 = one_hot(df3,categoricals)\n",
    "\n",
    "df3 = df3.drop(categoricals, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelos\n",
    "\n",
    "\n",
    "Veremos agora alguns modelos bastante simples com suas próprias caracteristicas que podemos testar e combina-los para conseguir tirar o melhor previsor da combinação dos modelos (método de ensemble). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes de mais nada criamos algumas funções de apoio para medirmos o desempenho desses modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abaixo a função de split que divide nosso dataframe em 2 partes de forma aleatória, uma para treinarmos os modelos e outra para testarmos. É especialmente importante que não usemos os dados de treinamento para testar pois se nosso modelo ficar perfeitamente bom para os dados treinados pode ser que ele não seja genérico o suficiente para ser extrapolado para outros dados, a separação nessa etapa tem esse proposóto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfSplit(df,ratio,y='log_price'):\n",
    "    train, test = train_test_split(df, test_size = ratio)\n",
    "    y_train = train[y]\n",
    "    y_test = test[y]\n",
    "    x_train = train.ix[:, train.columns != y]\n",
    "    x_test = test.ix[:, test.columns != y]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Após criar e ajustar diversas variaveis nas etapas anteriores, ainda assim podemos criar polinomios para que nossa função obtenha a flexibilidade de:\n",
    "\n",
    "\n",
    "\n",
    "## (1) se ajustar a funções quadraticas e principalmente tornar os parametros marginalmente decrescente, ou seja, tem sua primeira derivada positiva e a segunda negativa e \n",
    "\n",
    "\n",
    "\n",
    "## (2) permitir interação entre os efeitos, ou seja, será que o efeito do aumento do tamanho da casa se mantém constante quando outras variaveis variam? Acho que não, e podemos testar essas interações criando polinomios multiplicativos.\n",
    "\n",
    "\n",
    "\n",
    "obs: Essa inserção de parametros multiplicativos não quebra a constistência e robustez da OLS mesmo ela sendo 'linear', pois os parâmetros ajustados (os betas) continuam sendo lineares!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tPoly(df, degree=1):\n",
    "    polynomial = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    return polynomial.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de ajustes.\n",
    "\n",
    "\n",
    "#### Diferente das métricas de classificação (precisão, recall, f1-score, etc) as métricas de regressões são um pouco mais complicadas e trabalharemos com as mais simples, a RMSE que tem seus defeitos mas será útil nesse caso e nosso R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "\n",
    "def testRegs(df, clf, degree=1, ratio=.2, y='log_price', metrics=[]):\n",
    "\n",
    "    x_train,y_train,x_test,y_test = dfSplit(df,ratio,y='log_price')\n",
    "\n",
    "    poly_x_train = tPoly(x_train,degree)\n",
    "    poly_x_test = tPoly(x_test,degree)\n",
    "\n",
    "    clf.fit(poly_x_train,y_train)\n",
    "    \n",
    "    y_hat = clf.predict(poly_x_test)\n",
    "    \n",
    "    rmse = np.sqrt(-cross_val_score(clf, poly_x_train, y_train, scoring = scorer, cv = 10))\n",
    "    \n",
    "    print(\"RMSE:\", rmse.mean())\n",
    "    \n",
    "    y_train_pred = clf.predict(poly_x_train)\n",
    "    y_test_pred = clf.predict(poly_x_test)\n",
    "\n",
    "    print('R2: %.2f, Score: %.2f, Parameters: %i' % (r2_score(y_test, y_hat), \n",
    "                                                     clf.score(poly_x_train,y_train), \n",
    "                                                     clf.coef_.shape[0]))\n",
    "\n",
    "    plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "    plt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "    plt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Real values\")\n",
    "    plt.legend(loc = \"upper left\")\n",
    "    plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrediremos com OLS os dados sobre diferentes valores polinomias para ter uma noção geral de sobreajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ols_simple = linear_model.LinearRegression()\n",
    "\n",
    "for i in range(1,3):\n",
    "    testRegs(df3,ols_simple,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como sabemos que nossa base de dados tem bastante multicolinearidade entre os parametros, vamos usar regressões robustas a isso para minimizar esses efeitos de overfiting.\n",
    "\n",
    "Nessas regressões com regularização, basicamente é adicionado viés aos parâmetros para penalizar os efeitos de valores extremos. Ridge usa L2 e Lasso usa L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge = linear_model.RidgeCV(alphas = [0.001, 0.01, 0.1, 0.5, 0.75, 1, 1.2, 1.5, 2.5, 5])\n",
    "testRegs(df3,ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003,\n",
    "                                       0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],\n",
    "                                       max_iter = 50000, cv = 10)\n",
    "testRegs(df3,lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = ensemble.GradientBoostingRegressor(n_estimators = 500, \n",
    "                                              max_depth = 5, \n",
    "                                              min_samples_split = 2)\n",
    "\n",
    "x_train, y_train, x_test, y_test = dfSplit(df3,.1)\n",
    "gbr = ensemble.fit(x_train, y_train)\n",
    "print(ensemble.score(x_test, y_test))\n",
    "print(np.sqrt(-cross_val_score(gbr, x_train, y_train, scoring = scorer, cv = 5))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Inserindo geolocalização como variaveis autocorrelacionadas endógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=df.longitude, y=df.latitude, size=8, kind='kde', space=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "mses = pd.Series({'OLS': mse(y, m1.predy.flatten()), \\\n",
    "                     'OLS+W': mse(y, m2.predy.flatten()), \\\n",
    "                     'Lag': mse(y, m3.predy_e)\n",
    "                    })\n",
    "mses.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma possibilidade que está bastante na moda é usar Deep learning (nne com camadas ocultas), eu honestamente acho uma ótima ideia para diversos problemas como imagens, sons e outros problemas não linearizaveis, mas o fato de podermos escrever uma função qualquer  não linearizavel quando sabemos que a função deveria ter parâmetros lineares só abre a possibilidade de overfiting desnecesário.\n",
    "\n",
    "Abaixo alguns casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=42, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=simple_model, nb_epoch=10000, batch_size=1000, verbose=False)\n",
    "\n",
    "x_train, y_train, x_test, y_test = dfSplit(df3,.1)\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(estimator, x_train.values, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_model_with_hidden():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=42, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn=simple_model_with_hidden, nb_epoch=10000, batch_size=1000, verbose=False)\n",
    "\n",
    "x_train, y_train, x_test, y_test = dfSplit(df3,.01)\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(estimator, x_train.values, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
